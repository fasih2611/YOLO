import gc
import sys
import os
import time
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import onnxruntime
from ultralytics import YOLO
import logging
from statistics import mean
import numpy as np
from PIL import Image
import csv
import json
from collections import defaultdict
sys.path.append('./YOLOv8-test')
from ultralytics.utils.ops import non_max_suppression # type: ignore
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# Global variables remain the same
batch_size = 1
class_no = 80
preprocess_time = 0

class COCODataset(Dataset):
    def __init__(self, image_folder, input_size):
        self.image_folder = image_folder
        self.input_size = input_size
        self.transform = transforms.Compose([
            transforms.Resize((input_size, input_size)),
            transforms.ToTensor(),
        ])

        # Filter out grayscale images during initialization
        self.valid_images = []
        skipped_images = []

        for f in os.listdir(image_folder):
            if f.endswith(('.png', '.jpg', '.jpeg')):
                try:
                    image_path = os.path.join(image_folder, f)
                    with Image.open(image_path) as img:
                        if img.mode == 'L':  # L means grayscale
                            skipped_images.append(f)
                            continue
                        if img.mode != 'RGB':
                            img = img.convert('RGB')
                        self.valid_images.append(f)
                except Exception as e:
                    logging.error(f"Error loading image {f}: {str(e)}")
                    continue



        if skipped_images:
            logging.info(f"Skipped {len(skipped_images)} grayscale images: {', '.join(skipped_images)}")
            print(f"Skipped {len(skipped_images)} grayscale images. Check log for details.")

        print(f"Found {len(self.valid_images)} valid RGB images")

    def __len__(self):
        return len(self.valid_images)

    def __getitem__(self, idx):
        start_time = time.perf_counter()

        image_path = os.path.join(self.image_folder, self.valid_images[idx])
        try:
            with Image.open(image_path) as image:
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                tensor = self.transform(image)

                end_time = time.perf_counter()
                preprocess_duration = end_time - start_time

                return tensor, self.valid_images[idx], preprocess_duration
        except Exception as e:
            logging.error(f"Error processing image {self.valid_images[idx]} during batch loading: {str(e)}")
            tensor = torch.zeros(3, self.input_size, self.input_size)
            return tensor, self.valid_images[idx], 0.0

class COCOEvaluator:
    def __init__(self, annotation_file):
        with open(annotation_file, 'r') as f:
            self.dataset = json.load(f)

        # YOLO class mapping
        self.yolo_classes = {
            0: "person", 1: "bicycle", 2: "car", 3: "motorcycle", 4: "airplane",
            5: "bus", 6: "train", 7: "truck", 8: "boat", 9: "traffic light",
            10: "fire hydrant", 11: "stop sign", 12: "parking meter", 13: "bench",
            14: "bird", 15: "cat", 16: "dog", 17: "horse", 18: "sheep", 19: "cow",
            20: "elephant", 21: "bear", 22: "zebra", 23: "giraffe", 24: "backpack",
            25: "umbrella", 26: "handbag", 27: "tie", 28: "suitcase", 29: "frisbee",
            30: "skis", 31: "snowboard", 32: "sports ball", 33: "kite", 34: "baseball bat",
            35: "baseball glove", 36: "skateboard", 37: "surfboard", 38: "tennis racket",
            39: "bottle", 40: "wine glass", 41: "cup", 42: "fork", 43: "knife",
            44: "spoon", 45: "bowl", 46: "banana", 47: "apple", 48: "sandwich",
            49: "orange", 50: "brocolli", 51: "carrot", 52: "hot dog", 53: "pizza",
            54: "donut", 55: "cake", 56: "chair", 57: "couch", 58: "potted plant",
            59: "bed", 60: "dining table", 61: "toilet", 62: "tv", 63: "laptop",
            64: "mouse", 65: "remote", 66: "keyboard", 67: "cell phone", 68: "microwave",
            69: "oven", 70: "toaster", 71: "sink", 72: "refrigerator", 73: "book",
            74: "clock", 75: "vase", 76: "scissors", 77: "teddy bear", 78: "hair drier",
            79: "toothbrush"
        }

        # Create reverse mapping from class name to YOLO index
        self.yolo_class_to_idx = {v: k for k, v in self.yolo_classes.items()}

        # Create mapping from COCO category ID to YOLO index
        self.coco_to_yolo = {}
        for cat in self.dataset['categories']:
            if cat['name'] in self.yolo_class_to_idx:
                self.coco_to_yolo[cat['id']] = self.yolo_class_to_idx[cat['name']]

        # Create lookup dictionaries
        self.images = {img['file_name']: img for img in self.dataset['images']}
        self.categories = {cat['id']: cat for cat in self.dataset['categories']}

        # Print category mapping for debugging
        print("\nCategory Mapping (COCO → YOLO):")
        for coco_id, cat in self.categories.items():
            yolo_idx = self.coco_to_yolo.get(coco_id, "N/A")
            print(f"COCO ID: {coco_id:3d}, Name: {cat['name']:20s} → YOLO idx: {yolo_idx}")

        # Create ground truth annotations index
        self.gt_annotations = defaultdict(list)
        for ann in self.dataset['annotations']:
            img_id = ann['image_id']
            self.gt_annotations[img_id].append(ann)

        # Print annotation statistics
        print(f"\nTotal images in annotations: {len(self.images)}")
        print(f"Total categories: {len(self.categories)}")
        print(f"Total annotations: {len(self.dataset['annotations'])}")
        print(f"Mapped YOLO categories: {len(self.coco_to_yolo)}")

    def evaluate(self, predictions, image_files, input_size):
        """
        Evaluate predictions against ground truth annotations
        """
        metric = MeanAveragePrecision(box_format='xywh',max_detection_thresholds=[1,10,100])

        # Debug counters

        for img_idx, (image_file, pred) in enumerate(zip(image_files, predictions)):
            if image_file not in self.images:
                print(f"\nWarning: Image {image_file} not found in annotations")
                continue

            img_info = self.images[image_file]
            img_id = img_info['id']

            # Get original image dimensions for scaling
            orig_h, orig_w = img_info['height'], img_info['width']
            scale_w = orig_w / input_size
            scale_h = orig_h / input_size


            if pred is None or len(pred) == 0:
                continue

            pred_boxes = []
            pred_scores = []
            pred_labels = []

            for p_idx, p in enumerate(pred):
                x1, y1, x2, y2, conf, cls = p.tolist()
                # Scale coordinates back to original image size
                x1 *= scale_w
                x2 *= scale_w
                y1 *= scale_h
                y2 *= scale_h

                # Convert to [x, y, w, h] format
                width = x2 - x1
                height = y2 - y1

                pred_boxes.append([x1, y1, width, height])
                pred_scores.append(conf)
                pred_labels.append(int(cls))


            # Get ground truth boxes
            gt_boxes = []
            gt_labels = []

            gt_anns = self.gt_annotations[img_id]


            for ann_idx, ann in enumerate(gt_anns):
                coco_category_id = ann['category_id']
                if coco_category_id not in self.coco_to_yolo:
                    continue

                yolo_class_idx = self.coco_to_yolo[coco_category_id]
                gt_boxes.append(ann['bbox'])
                gt_labels.append(yolo_class_idx)

            if len(pred_boxes) > 0 and len(gt_boxes) > 0:
                metric.update(
                    preds=[{
                        'boxes': torch.tensor(pred_boxes),
                        'scores': torch.tensor(pred_scores),
                        'labels': torch.tensor(pred_labels),
                    }],
                    target=[{
                        'boxes': torch.tensor(gt_boxes),
                        'labels': torch.tensor(gt_labels),
                    }]
                )

        # Print summary statistics
        #print("\nEvaluation Summary:")
        #print(f"Processed images: {processed_images}")
        #print(f"Images with predictions: {images_with_predictions}")
        #print(f"Images with ground truth: {images_with_gt}")
        #print(f"Images with both pred & GT: {images_with_both}")
        #print(f"Total predictions: {total_predictions}")
        #print(f"Total GT boxes: {total_gt_boxes}")
        #print(f"Skipped GT boxes (unmapped categories): {skipped_gt_boxes}")
        #print(f"Average predictions per image: {total_predictions/processed_images:.2f}")
        #print(f"Average GT boxes per image: {total_gt_boxes/processed_images:.2f}")

        #print("\nPredicted Class Distribution:")
        #for cls_id, count in sorted(class_distribution.items()):
        #    if cls_id in self.yolo_classes:
        #        print(f"Class {cls_id} ({self.yolo_classes[cls_id]}): {count}")
        #    else:
        #        print(f"Class {cls_id} (unknown): {count}")

        metrics = metric.compute()
        print("\nMetrics:")
        from pprint import pprint
        pprint(metrics)

        return metrics['map_50'], metrics['map']

def calculate_precision(predictions, coco_evaluator, image_files, input_size):
    """Wrapper function to maintain compatibility with existing code"""
    return coco_evaluator.evaluate(predictions, image_files, input_size)

def inference_ultralytics(model, dataloader):
    def model_func(batch):
        return model.model(batch)

    return run_inference(model_func, dataloader)

def inference_onnx(onnx_path, dataloader):
    session = onnxruntime.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])
    input_name = session.get_inputs()[0].name

    def model_func(batch):
        return session.run(None, {input_name: batch.numpy()})

    return run_inference(model_func, dataloader)


def write_results_to_csv(results, filename='yolo_precision_results.csv'):
    with open(filename, 'w', newline='') as csvfile:
        fieldnames = ['Model', 'Image Size', 'Preprocess Time', 'Inference Time', 'Post-processing Time', 'mAP@0.5', 'mAP@0.5:0.95']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for image_size, size_results in results.items():
            for model, model_results in size_results.items():
                writer.writerow({
                    'Model': model,
                    'Image Size': image_size,
                    'Preprocess Time': f'{model_results[0]:.4f}',
                    'Inference Time': f'{model_results[1]:.4f}',
                    'Post-processing Time': f'{model_results[2]:.4f}',
                    'mAP@0.5': f'{model_results[3]:.4f}',
                    'mAP@0.5:0.95': f'{model_results[4]:.4f}'
                })


def run_inference(model_func, dataloader):
    inference_times = []
    post_processing_times = []
    preprocess_times = []
    all_predictions = []
    image_files = []

    for batch, filenames, prep_times in tqdm(dataloader, desc="Running inference"):
        preprocess_times.extend(prep_times.tolist())

        # Inference
        start_time = time.perf_counter()
        pred = model_func(batch)
        end_time = time.perf_counter()
        inference_times.append(end_time - start_time)
        # Post-processing
        start_time = time.perf_counter()
        # Note: non_max_suppression expects and returns boxes in [x1, y1, x2, y2] format
        processed_pred = non_max_suppression(torch.tensor(pred[0]) if not isinstance(pred, torch.Tensor) else pred,0.001,0.7)


        end_time = time.perf_counter()
        post_processing_times.append(end_time - start_time)

        all_predictions.extend(processed_pred)
        image_files.extend(filenames)

    avg_preprocess_time = mean(preprocess_times)
    avg_inference_time = mean(inference_times)
    avg_post_processing_time = mean(post_processing_times)

    return avg_preprocess_time, avg_inference_time, avg_post_processing_time, all_predictions, image_files


# Add these debug functions to help diagnose prediction issues
def print_prediction_summary(predictions, image_ids):
    print("\nPrediction Summary:")
    for i, pred in enumerate(predictions):
        if pred is not None and len(pred) > 0:
            print(f"Image {image_ids[i]}: {len(pred)} predictions")
            print("Sample prediction:", pred[0])

def run_comparisons(input_size, model_type):
    print(f"\nRunning comparisons for {input_size}x{input_size} images:")

    # Initialize evaluator with COCO annotations
    coco_evaluator = COCOEvaluator('./datasets/coco/labels/annotations/instances_val2017.json')

    # Rest of the function remains the same...
    dataset = COCODataset('./datasets/coco/images/val2017', input_size)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    ultralytics_model = YOLO(f'{model_type}.pt')
    ultralytics_model.fuse()

    ultralytics_model.export(format="onnx", batch=batch_size,
                           imgsz=input_size, simplify=True, opset=13)

    ultralytics_results = inference_ultralytics(ultralytics_model, dataloader)
    ultralytics_map50, ultralytics_map = calculate_precision(
        ultralytics_results[3], coco_evaluator, ultralytics_results[4], input_size)

    ultralytics_onnx_results = inference_onnx(f'./{model_type}.onnx', dataloader)
    ultralytics_onnx_map50, ultralytics_onnx_map = calculate_precision(
        ultralytics_onnx_results[3], coco_evaluator, ultralytics_onnx_results[4], input_size)

    del ultralytics_model
    gc.collect()

    return {
        f'{model_type} ultralytics': ultralytics_results[:3] + (ultralytics_map50, ultralytics_map),
        f'{model_type} ultralytics_onnx': ultralytics_onnx_results[:3] + (ultralytics_onnx_map50, ultralytics_onnx_map),
    }

def main():
    sizes = [640]
    models = ['yolov5n6u', 'yolov8n', 'yolov9t','yolov10n', 'yolo11n']
    results = {}
    for model in models:
        for size in sizes:
            results[size] = run_comparisons(size, model)
        write_results_to_csv(results, filename=f'{model} - precision comparison.csv')

if __name__ == "__main__":
    main()