{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T07:08:10.412787Z","iopub.status.busy":"2024-09-09T07:08:10.411921Z","iopub.status.idle":"2024-09-09T07:08:10.417312Z","shell.execute_reply":"2024-09-09T07:08:10.416314Z","shell.execute_reply.started":"2024-09-09T07:08:10.412739Z"},"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('./YOLOv8-test')"]},{"cell_type":"markdown","metadata":{},"source":["### Inference test\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/muhammadfasi/Downloads/YOLOV8/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import yaml\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","import time\n","import os\n","from tqdm import tqdm\n","from nets import nn\n","from utils.util import non_max_suppression\n","import onnx\n","import onnxruntime\n","from deepsparse import Engine\n","from ultralytics import YOLO"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T07:21:33.222722Z","iopub.status.busy":"2024-09-09T07:21:33.222281Z","iopub.status.idle":"2024-09-09T07:21:49.415646Z","shell.execute_reply":"2024-09-09T07:21:49.414650Z","shell.execute_reply.started":"2024-09-09T07:21:33.222683Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Running comparisons for 1280x1280 images:\n","Preprocessing Time per image (1280x1280): 2.4218 seconds\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_40088/3718253806.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(weights_path, map_location='cpu')\n","/home/muhammadfasi/Downloads/YOLOV8/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(file, map_location='cpu'), file  # load\n","YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n","Ultralytics YOLOv8.0.124 ðŸš€ Python-3.11.9 torch-2.4.0+cu124 CPU\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8n.pt with input shape (1, 3, 1280, 1280) BCHW and output shape(s) (1, 84, 33600) (6.2 MB)\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 13...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.4s, saved as yolov8n.onnx (12.7 MB)\n","\n","Export complete (0.8s)\n","Results saved to \u001b[1m/home/muhammadfasi/Downloads/YOLOV8\u001b[0m\n","Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=1280 \n","Validate:        yolo val task=detect model=yolov8n.onnx imgsz=1280 data=coco.yaml \n","Visualize:       https://netron.app\n","Processing images (PyTorch CPU): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:15<00:00,  8.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (PyTorch CPU): 15.6868 seconds\n","Average Post-Processing Time (PyTorch CPU): 0.0290 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (ONNX): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:19<00:00,  6.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (ONNX): 17.0578 seconds\n","Average Post-Processing Time (ONNX): 2.5141 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:06<00:00, 18.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (DeepSparse): 6.5325 seconds\n","Average Post-Processing Time (DeepSparse): 0.1813 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:14<00:00,  8.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (Ultralytics.pt): 14.5750 seconds\n","Average Post-Processing Time (Ultralytics.pt): 0.1403 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (ONNX): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:18<00:00,  6.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (ONNX): 16.6907 seconds\n","Average Post-Processing Time (ONNX): 1.5047 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:06<00:00, 18.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (DeepSparse): 6.5735 seconds\n","Average Post-Processing Time (DeepSparse): 0.1786 seconds\n","\n","Running comparisons for 640x640 images:\n","Preprocessing Time per image (640x640): 0.4791 seconds\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_40088/3718253806.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(weights_path, map_location='cpu')\n","/home/muhammadfasi/Downloads/YOLOV8/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(file, map_location='cpu'), file  # load\n","YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n","Ultralytics YOLOv8.0.124 ðŸš€ Python-3.11.9 torch-2.4.0+cu124 CPU\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8n.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 13...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.3s, saved as yolov8n.onnx (12.2 MB)\n","\n","Export complete (0.3s)\n","Results saved to \u001b[1m/home/muhammadfasi/Downloads/YOLOV8\u001b[0m\n","Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=640 \n","Validate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml \n","Visualize:       https://netron.app\n","Processing images (PyTorch CPU): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 33.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (PyTorch CPU): 3.7760 seconds\n","Average Post-Processing Time (PyTorch CPU): 0.0175 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (ONNX): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:05<00:00, 24.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (ONNX): 3.8394 seconds\n","Average Post-Processing Time (ONNX): 1.3650 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:02<00:00, 51.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (DeepSparse): 2.4012 seconds\n","Average Post-Processing Time (DeepSparse): 0.0647 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 39.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (Ultralytics.pt): 3.1928 seconds\n","Average Post-Processing Time (Ultralytics.pt): 0.0320 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (ONNX): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:05<00:00, 22.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (ONNX): 3.8457 seconds\n","Average Post-Processing Time (ONNX): 1.7374 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:02<00:00, 48.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (DeepSparse): 2.5676 seconds\n","Average Post-Processing Time (DeepSparse): 0.0598 seconds\n","\n","Running comparisons for 256x256 images:\n","Preprocessing Time per image (256x256): 0.3121 seconds\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_40088/3718253806.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(weights_path, map_location='cpu')\n","/home/muhammadfasi/Downloads/YOLOV8/.conda/lib/python3.11/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(file, map_location='cpu'), file  # load\n","YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n","Ultralytics YOLOv8.0.124 ðŸš€ Python-3.11.9 torch-2.4.0+cu124 CPU\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8n.pt with input shape (1, 3, 256, 256) BCHW and output shape(s) (1, 84, 1344) (6.2 MB)\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 13...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.2s, saved as yolov8n.onnx (12.1 MB)\n","\n","Export complete (0.3s)\n","Results saved to \u001b[1m/home/muhammadfasi/Downloads/YOLOV8\u001b[0m\n","Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=256 \n","Validate:        yolo val task=detect model=yolov8n.onnx imgsz=256 data=coco.yaml \n","Visualize:       https://netron.app\n","Processing images (PyTorch CPU): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:01<00:00, 89.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (PyTorch CPU): 1.3930 seconds\n","Average Post-Processing Time (PyTorch CPU): 0.0104 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (ONNX): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 35.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (ONNX): 1.1541 seconds\n","Average Post-Processing Time (ONNX): 2.4086 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:01<00:00, 78.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (DeepSparse): 1.5794 seconds\n","Average Post-Processing Time (DeepSparse): 0.0244 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:01<00:00, 98.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (Ultralytics.pt): 1.2745 seconds\n","Average Post-Processing Time (Ultralytics.pt): 0.0151 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (ONNX): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 35.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (ONNX): 1.2994 seconds\n","Average Post-Processing Time (ONNX): 2.2389 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Processing images (DeepSparse): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:01<00:00, 70.47it/s]"]},{"name":"stdout","output_type":"stream","text":["Average Inference Time (DeepSparse): 1.7650 seconds\n","Average Post-Processing Time (DeepSparse): 0.0296 seconds\n","\n","Comparison Results:\n","640x640 Images:\n","pytorch_cpu: Inference Time = 3.7760s, Post-processing Time = 0.0175s\n","onnx: Inference Time = 3.8394s, Post-processing Time = 1.3650s\n","deepsparse: Inference Time = 2.4012s, Post-processing Time = 0.0647s\n","ultralytics: Inference Time = 3.1928s, Post-processing Time = 0.0320s\n","ultralytics_onnx: Inference Time = 3.8457s, Post-processing Time = 1.7374s\n","ultralytics_deepsparse: Inference Time = 2.5676s, Post-processing Time = 0.0598s\n","\n","256x256 Images:\n","pytorch_cpu: Inference Time = 1.3930s, Post-processing Time = 0.0104s\n","onnx: Inference Time = 1.1541s, Post-processing Time = 2.4086s\n","deepsparse: Inference Time = 1.5794s, Post-processing Time = 0.0244s\n","ultralytics: Inference Time = 1.2745s, Post-processing Time = 0.0151s\n","ultralytics_onnx: Inference Time = 1.2994s, Post-processing Time = 2.2389s\n","ultralytics_deepsparse: Inference Time = 1.7650s, Post-processing Time = 0.0296s\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["global batch_size, class_no\n","batch_size = 1\n","class_no = 80\n","torch.jit.enable_onednn_fusion(True)\n","def load_custom_model(weights_path, num_classes):\n","    model = nn.yolo_v8_n(num_classes).cpu()\n","    ckpt = torch.load(weights_path, map_location='cpu')\n","    model.load_state_dict(ckpt['model'].float().state_dict(), strict=False)\n","    model.eval()\n","    return model.fuse()\n","\n","\n","def preprocess_images(image_folder, input_size):\n","    resize_transform = transforms.Compose([\n","        transforms.Resize((input_size, input_size)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n","    batches = []\n","    \n","    preprocess_time = 0  # Timer for pre-processing\n","\n","    for i in range(0, len(image_files), batch_size):\n","        batch = []\n","        for j in range(i, min(i + batch_size, len(image_files))):\n","            start_time = time.time()\n","            image = Image.open(os.path.join(image_folder, image_files[j]))\n","            tensor = resize_transform(image)\n","            tensor = tensor / 255\n","            end_time = time.time()\n","            preprocess_time += end_time - start_time\n","            \n","            batch.append(tensor)\n","        batches.append(torch.stack(batch))\n","    \n","    print(f\"Preprocessing Time per image ({input_size}x{input_size}): {preprocess_time/batch_size:.4f} seconds\")\n","    return batches\n","\n","@torch.inference_mode()\n","def inference_pytorch_cpu(model, batches, num_warmup=5):\n","    # Warm-up runs\n","    torch.compile(model)\n","    for _ in range(num_warmup):\n","        with torch.no_grad():\n","            _ = model(batches[0])\n","    \n","    total_time = 0\n","    post_processing_time = 0  # Timer for post-processing\n","    \n","    for batch in tqdm(batches, desc=\"Processing images (PyTorch CPU)\"):\n","        with torch.no_grad():\n","            start_time = time.time()\n","            pred = model(batch)\n","            end_time = time.time()\n","            inference_time = end_time - start_time\n","            total_time += inference_time\n","            \n","            # Post-processing (if any)\n","            start_time = time.time()\n","            pred = non_max_suppression(pred,classes=class_no)\n","            end_time = time.time()\n","            post_processing_time += end_time - start_time\n","            \n","    avg_inference_time = total_time / batch_size\n","    avg_post_processing_time = post_processing_time / batch_size\n","    \n","    print(f\"Average Inference Time (PyTorch CPU): {avg_inference_time:.4f} seconds\")\n","    print(f\"Average Post-Processing Time (PyTorch CPU): {avg_post_processing_time:.4f} seconds\")\n","    \n","    return avg_inference_time, avg_post_processing_time\n","\n","\n","def inference_onnx(onnx_path, batches, num_warmup=5):\n","    session = onnxruntime.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n","    input_name = session.get_inputs()[0].name\n","\n","    # Warm-up runs\n","    for _ in range(num_warmup):\n","        _ = session.run(None, {input_name: batches[0].numpy()})\n","\n","    total_time = 0\n","    post_processing_time = 0  # Timer for post-processing\n","    \n","    for batch in tqdm(batches, desc=\"Processing images (ONNX)\"):\n","        start_time = time.time()\n","        pred = session.run(None, {input_name: batch.numpy()})\n","        end_time = time.time()\n","        inference_time = end_time - start_time\n","        total_time += inference_time\n","        \n","        # Post-processing (if any)\n","        start_time = time.time()\n","        non_max_suppression(torch.tensor(pred[0]),classes=class_no)\n","        end_time = time.time()\n","        post_processing_time += end_time - start_time\n","        \n","    avg_inference_time = total_time / batch_size\n","    avg_post_processing_time = post_processing_time / batch_size\n","    \n","    print(f\"Average Inference Time (ONNX): {avg_inference_time:.4f} seconds\")\n","    print(f\"Average Post-Processing Time (ONNX): {avg_post_processing_time:.4f} seconds\")\n","    \n","    return avg_inference_time, avg_post_processing_time\n","\n","def inference_deepsparse(onnx_path, batches, num_warmup=5):\n","    pipe = Engine(onnx_path, batch_size=batch_size)\n","\n","    for _ in range(num_warmup):\n","        _ = pipe([batches[0].numpy()])\n","\n","    total_time = 0\n","    post_processing_time = 0  # Timer for post-processing\n","    \n","    for batch in tqdm(batches, desc=\"Processing images (DeepSparse)\"):\n","        start_time = time.time()\n","        pred = pipe([batch.numpy()])\n","        end_time = time.time()\n","        inference_time = end_time - start_time\n","        total_time += inference_time\n","        \n","        start_time = time.time()\n","        non_max_suppression(torch.tensor(pred[0]),classes=class_no)\n","        end_time = time.time()\n","        post_processing_time += end_time - start_time\n","        \n","    avg_inference_time = total_time / batch_size\n","    avg_post_processing_time = post_processing_time / batch_size\n","    \n","    print(f\"Average Inference Time (DeepSparse): {avg_inference_time:.4f} seconds\")\n","    print(f\"Average Post-Processing Time (DeepSparse): {avg_post_processing_time:.4f} seconds\")\n","    \n","    return avg_inference_time, avg_post_processing_time\n","\n","def inference_ultralytics(model, batches, num_warmup=5):\n","    # Warm-up runsYOLOv8-test/weights/yolov8_model_slim.onnx\n","\n","    for _ in range(num_warmup):\n","        _ = model.model(batches[0])\n","\n","    total_time = 0\n","    post_processing_time = 0  # Timer for post-processing\n","    \n","    for batch in tqdm(batches, desc=\"Processing images (DeepSparse)\"):\n","        start_time = time.time()\n","        pred = model.model(batch)\n","        end_time = time.time()\n","        inference_time = end_time - start_time\n","        total_time += inference_time\n","        \n","        start_time = time.time()\n","        non_max_suppression(torch.tensor(pred[0]),classes=class_no)\n","        end_time = time.time()\n","        post_processing_time += end_time - start_time\n","        \n","    avg_inference_time = total_time / batch_size\n","    avg_post_processing_time = post_processing_time / batch_size\n","    \n","    print(f\"Average Inference Time (Ultralytics.pt): {avg_inference_time:.4f} seconds\")\n","    print(f\"Average Post-Processing Time (Ultralytics.pt): {avg_post_processing_time:.4f} seconds\")\n","    \n","    return avg_inference_time, avg_post_processing_time\n","\n","def run_comparisons(input_size):\n","    print(f\"\\nRunning comparisons for {input_size}x{input_size} images:\")\n","    \n","    # Prepare data\n","    batches = preprocess_images('./datasets/coco128/images/train2017', input_size)\n","\n","    # Your custom YOLOv8 implementations\n","    dummy_input = torch.randn(1, 3, input_size, input_size)  \n","    custom_model = load_custom_model('./YOLOv8-test/weights/v8_n(1).pt', class_no)\n","\n","    onnx_path_custom = f'./YOLOv8-test/weights/yolov8_custom_{input_size}.onnx'\n","    torch.onnx.export(custom_model, \n","                  dummy_input, \n","                  onnx_path_custom,\n","                  opset_version=13,\n","                  input_names=['input'],\n","                  output_names=['output'],\n","                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n","    \n","    # Ultralytics YOLOv8\n","    ultralytics_model = YOLO('yolov8n.pt')\n","    ultralytics_model.fuse()\n","    ultralytics_model.export(format=\"onnx\", batch=1, imgsz=input_size,opset=13)\n","\n","    # Run inferences with warm-up\n","    num_warmup = 10  # Number of warm-up runs\n","    pytorch_cpu_time = inference_pytorch_cpu(custom_model, batches, num_warmup)\n","    onnx_time = inference_onnx(onnx_path_custom, batches, num_warmup)\n","    deepsparse_time = inference_deepsparse(onnx_path_custom, batches, num_warmup)\n","    ultralytics_time = inference_ultralytics(ultralytics_model, batches, num_warmup)\n","    ultralytics_onnx_time = inference_onnx(f'./yolov8n.onnx', batches, num_warmup)\n","    ultralytics_deepsparse = inference_deepsparse(f'./yolov8n.onnx', batches, num_warmup)\n","\n","    return {\n","        'pytorch_cpu': pytorch_cpu_time,\n","        'onnx': onnx_time,\n","        'deepsparse': deepsparse_time,\n","        'ultralytics': ultralytics_time,\n","        'ultralytics_onnx': ultralytics_onnx_time,\n","        'ultralytics_deepsparse': ultralytics_deepsparse\n","    }\n","\n","def main():\n","    with open('./YOLOv8-test/utils/args.yaml', errors='ignore') as f:\n","        params = yaml.safe_load(f)\n","\n","    results_1280 = run_comparisons(1280)\n","    results_640 = run_comparisons(640)\n","    results_256 = run_comparisons(256)\n","    \n","\n","    print(\"\\nComparison Results:\")\n","    print(\"640x640 Images:\")\n","    for model, times in results_640.items():\n","        print(f\"{model}: Inference Time = {times[0]:.4f}s, Post-processing Time = {times[1]:.4f}s\")\n","    \n","    print(\"\\n256x256 Images:\")\n","    for model, times in results_256.items():\n","        print(f\"{model}: Inference Time = {times[0]:.4f}s, Post-processing Time = {times[1]:.4f}s\")\n","\n","if __name__ == \"__main__\":\n","    main()  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T13:24:42.664395Z","iopub.status.busy":"2024-09-04T13:24:42.663672Z","iopub.status.idle":"2024-09-04T13:24:58.572926Z","shell.execute_reply":"2024-09-04T13:24:58.571679Z","shell.execute_reply.started":"2024-09-04T13:24:42.664354Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: onnxruntime in ./.conda/lib/python3.11/site-packages (1.19.0)\n","Requirement already satisfied: coloredlogs in ./.conda/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n","Requirement already satisfied: flatbuffers in ./.conda/lib/python3.11/site-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.6 in ./.conda/lib/python3.11/site-packages (from onnxruntime) (1.26.3)\n","Requirement already satisfied: packaging in ./.conda/lib/python3.11/site-packages (from onnxruntime) (24.1)\n","Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from onnxruntime) (5.27.3)\n","Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from onnxruntime) (1.12)\n","Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n","Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install onnxruntime"]},{"cell_type":"markdown","metadata":{},"source":["**Heatmap**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import yaml\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","import time\n","import os\n","from tqdm import tqdm\n","from nets import nn\n","from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt\n","\n","def load_custom_model(weights_path, num_classes):\n","    model = nn.yolo_v8_n(num_classes).cuda()\n","    ckpt = torch.load(weights_path, map_location='cuda')\n","    model.load_state_dict(ckpt['model'].float().state_dict())\n","    model.eval()\n","    return model.fuse().half()\n","\n","def preprocess_images(image_folder, input_size=640, batch_size=1):\n","    resize_transform = transforms.Compose([\n","        transforms.Resize((input_size, input_size)),\n","        transforms.ToTensor()\n","    ])\n","    \n","    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n","    batches = []\n","    \n","    for i in range(64, min(74, len(image_files)), batch_size):\n","        batch = []\n","        for j in range(i, min(i + batch_size, len(image_files))):\n","            if j == 62:\n","                continue\n","            image = Image.open(os.path.join(image_folder, image_files[j]))\n","            tensor = resize_transform(image)\n","            batch.append(tensor.half())\n","        \n","        batches.append(torch.stack(batch).cuda())\n","    return batches\n","\n","def create_heatmap(feature_map, writer, layer_name, global_step):\n","    feature_map = feature_map.squeeze().cpu().numpy().mean(axis=0)\n","    \n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(feature_map, cmap='hot', interpolation='nearest')\n","    plt.colorbar()\n","    plt.title(f'Heatmap of {layer_name}')\n","    plt.axis('off')\n","    \n","    # Save the figure as a static image\n","    output_dir = '/kaggle/working/heatmaps'\n","    os.makedirs(output_dir, exist_ok=True)\n","    plt.savefig(f'{output_dir}/{layer_name}_{global_step}.png')\n","    \n","    # Log to TensorBoard\n","    writer.add_figure(f'Heatmap/{layer_name}', plt.gcf(), global_step)\n","    plt.close()\n","\n","def inference(params):\n","    writer = SummaryWriter('/kaggle/working/tensorboard_logs')\n","    \n","    custom_model = load_custom_model('/kaggle/input/yolov8/pytorch/default/1/v8_n(1).pt', len(params['names']))\n","    \n","    batches = preprocess_images('/kaggle/input/coco128/coco128/images/train2017')\n","    \n","    # Warmup\n","    for _ in range(5):\n","        custom_model(torch.randn(1, 3, 640, 640).cuda().half())\n","    \n","    # Define hooks\n","    activation = {}\n","    def get_activation(name):\n","        def hook(model, input, output):\n","            activation[name] = output\n","        return hook\n","\n","    # Register hooks\n","    custom_model.net.p1.register_forward_hook(get_activation('p1'))\n","    custom_model.net.p2.register_forward_hook(get_activation('p2'))\n","    custom_model.net.p3.register_forward_hook(get_activation('p3'))\n","    custom_model.net.p4.register_forward_hook(get_activation('p4'))\n","    custom_model.net.p5.register_forward_hook(get_activation('p5'))\n","    \n","    global_step = 0\n","    for batch in tqdm(batches, desc=\"Processing images\"):\n","        with torch.no_grad():\n","            # Forward pass\n","            custom_model(batch)\n","            \n","            # Create heatmaps for each backbone layer output\n","            for layer_name in ['p1', 'p2', 'p3', 'p4', 'p5']:\n","                create_heatmap(activation[layer_name], writer, f'Backbone_{layer_name.upper()}', global_step)\n","        \n","        global_step += 1\n","    \n","    writer.close()\n","\n","def main():\n","    with open('/kaggle/working/YOLOv8-pt/utils/args.yaml', errors='ignore') as f:\n","        params = yaml.safe_load(f)\n","    \n","    inference(params)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%load_ext tensorboard\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import argparse\n","import torch\n","import yaml\n","from ultralytics import YOLO\n","from PIL import Image, ImageDraw\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","from nets import nn\n","from utils import util\n","\n","def load_custom_model(weights_path, num_classes):\n","    model = nn.yolo_v8_n(num_classes).cuda()\n","    ckpt = torch.load(weights_path, map_location='cuda')\n","    model.load_state_dict(ckpt['model'].float().state_dict())\n","    model  # Use half precision\n","    model.eval()\n","    return model\n","\n","def plot_boxes(image, boxes, color, label):\n","    draw = ImageDraw.Draw(image)\n","    for box in boxes:\n","        draw.rectangle(box, outline=color, width=2)\n","        draw.text((box[0], box[1]), label, fill=color)\n","    return image\n","\n","def inference(params):\n","    # Load models\n","    custom_model = load_custom_model('/kaggle/input/yolov8/pytorch/default/1/v8_n(1).pt', len(params['names']))\n","    yolov8n_model = YOLO('yolov8n.pt')\n","    # Load image\n","    image = Image.open('/kaggle/input/testin2/image.png')\n","    original_size = image.size\n","    \n","    # Resize image\n","    input_size = 640  # Standard input size for YOLOv8\n","    resize_transform = transforms.Compose([\n","        transforms.Resize((input_size, input_size)),\n","        transforms.ToTensor()\n","    ])\n","    \n","    # Resize for custom model\n","    custom_input = resize_transform(image).unsqueeze(0).cuda()\n","    \n","    # Resize for YOLOv8n (it expects a numpy array)\n","    yolov8n_input = resize_transform(image).permute(1, 2, 0).numpy()\n","    \n","    # Inference with custom model\n","    custom_output = custom_model(custom_input)\n","    custom_results = util.non_max_suppression(custom_output, 0.25, 0.7)\n","\n","    # Inference with YOLOv8n\n","    yolov8n_results = yolov8n_model.predict(custom_input)\n","\n","    # Print results\n","    print(\"Custom Model Results:\")\n","    print(custom_results)\n","    \n","    print(\"\\nYOLOv8n Results:\")\n","    print(yolov8n_results[0].boxes)\n","\n","    # Plot results on original image\n","    result_image = image.copy()\n","\n","    # Plot custom model results\n","    for det in custom_results[0]:\n","        box = det[:4].detach().cpu().numpy()\n","        # Rescale box to original image size\n","        box[0::2] *= original_size[0] / input_size\n","        box[1::2] *= original_size[1] / input_size\n","        result_image = plot_boxes(result_image, [box], \"red\", \"Custom\")\n","\n","    # Plot YOLOv8n results\n","    for box in yolov8n_results[0].boxes.xyxy:\n","        box = box.cpu().numpy()\n","        # Rescale box to original image size\n","        box[0::2] *= original_size[0] / input_size\n","        box[1::2] *= original_size[1] / input_size\n","        result_image = plot_boxes(result_image, [box], \"blue\", \"YOLOv8n\")\n","\n","    # Save the result image\n","    result_image.save('/kaggle/working/result.png')\n","    print(\"Result image saved as 'result.png'\")\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","\n","    with open('/kaggle/working/YOLOv8-pt/utils/args.yaml', errors='ignore') as f:\n","        params = yaml.safe_load(f)\n","    inference(params)\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":756977,"sourceId":4352009,"sourceType":"datasetVersion"},{"datasetId":5591376,"sourceId":9243179,"sourceType":"datasetVersion"},{"datasetId":5591406,"sourceId":9243223,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":108935,"modelInstanceId":84699,"sourceId":100991,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":109011,"modelInstanceId":84776,"sourceId":101095,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":114131,"modelInstanceId":89917,"sourceId":107321,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":114132,"modelInstanceId":89918,"sourceId":107322,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
